# Deep Learning Study Group

# 目的:
使用Pytorch, Tensorflow進行Deep Learning的實務學習
# 方法:
實務cases實作, 每週實體聚會討論交流, 隨時線上討論.
# 時程:
+ [X] Pytorch 入門: [Tensor 操作][16], torch optim...等 **2019/4/14**
  + [X] [Pytorch的backward()相关理解(舊版pytorch，參考)][17]
  + [X] [Pytorch基礎篇-張量、模型自動更新、優化器(舊版pytorch，參考)][18]
  + [X] [2019-0420 Slide: pyTorch - Get Started][21]@忠孝伯朗咖啡 
+ [X] Pytorch 101: Cases實作: 
  + [X] [世界人口預測 (Linear Regression)][6] **2019/5/5**
  + [X] [Stock Price (Logistic Regression)][7] **2019/5/5**
  + [X] [MNIST (Logistic Regression)][12] **2019/5/5**
  + [X] [2019-0505 Slide: pyTorch 101][23]@小樹屋-民權西路
+ [ ] Pytorch 201: Cases實作: 
  + [ ] [NLP: RNN 寫中國詩][24] **2019/5/19**
+ [ ] Tensorflow Cases實作: 
  + [ ] [Common knowledge][3] **2019/6/15**
  + [ ] [Case: 玩轉IRIS資料集, 零基礎範例][5] **2019/6/15**
  + [ ] [Case: Titanic survivor (Logistic Regression)][13] **2019/6/15**
  + [ ] [Case: Credit Card Analysis (Logistic Regression to DNN)][15] **2019/6/15**
  + [ ] [Case: MNIST (RNN)][2] **2019/6/29**
  + [ ] [Case: Fashion MNIST (DNN, GRU, Dataset API)][1] **2019/6/29**
  + [ ] [Case: Fashion MNIST (Tensorflow 2.0 API)][9] **2019/6/29**
+ [ ] Milestones目標 (關鍵成果):
  + [ ] [Pytorch: Seq2Seq 實作數學加法運算 (with or without Attention)][20] **2019/6/8**
  + [ ] [Pytorch: Attention is all you need (Transformer)][4] **2019/6/22**
  + [ ] [Tensorflow: Attention is all you need (Transformer)][8] **2019/7/13**
  + [ ] [Neural Ordinary Differential Equations][14] **2019/?/?**
    + [ ] [Ref: Neural ODE & DiffEqFlux - 杜岳華 (without QA)][22]
  + [ ] 大型資料黑客松競賽 **2019/11/1**
    + [ ] Search for global hackathon: https://www.hackathon.com/
      + [ ] possible event1: https://www.hackathon.com/event/take-europe-to-the-hearts-hackathon-39909500390
      + [ ] possible event2: https://www.eventbrite.com/e/zero-waste-retail-hackathon-tickets-58778354641
      + [ ] possible event3: https://techandrelationships.org/
  

## Reference Material
+ [Tensorflow 中文教學][10]
+ [Pytorch LSTM, GRU][19]
+ [Transformer 入門中文文章][11]

[1]:https://colab.research.google.com/drive/1Nn_9cdSK9yH4nWJx-vdKat8NWnmjopu0
[2]:https://colab.research.google.com/drive/18FqI18psdH30WUJ1uPd6zVgK2AwxO_Bj
[3]:https://medium.com/the-artificial-impostor/notes-understanding-tensorflow-part-1-5f0ebb253ad4
[4]:https://github.com/jadore801120/attention-is-all-you-need-pytorch
[5]:https://www.jianshu.com/p/b86c020747f9
[6]:https://github.com/ZhiqingXiao/pytorch-book/blob/master/chapter05_linear/population.ipynb
[7]:https://github.com/ZhiqingXiao/pytorch-book/blob/master/chapter06_logistic/stock_volume.ipynb
[8]:https://github.com/princewen/tensorflow_practice/tree/master/basic/Basic-Transformer-Demo
[9]:https://www.jianshu.com/p/c7a280600da8
[10]:https://github.com/Hvass-Labs/TensorFlow-Tutorials-Chinese
[11]:https://voidism.github.io/note/2019/02/05/Transformer_Intro/
[12]:https://medium.com/jovian-io/image-classification-using-logistic-regression-in-pytorch-ebb96cc9eb79
[13]:https://codability.in/a-guide-tensorflow-logistic-regression-part-6/
[14]:https://rkevingibson.github.io/blog/neural-networks-as-ordinary-differential-equations/
[15]:https://ipythonquant.wordpress.com/2018/06/20/from-logistic-regression-in-scikit-learn-to-deep-learning-with-tensorflow-a-fraud-detection-case-study-part-iii/
[16]:https://medium.com/jovian-io/pytorch-basics-tensors-and-gradients-eb2f6e8a6eee
[17]:https://blog.csdn.net/douhaoexia/article/details/78821428
[18]:https://fgc.stpi.narl.org.tw/activity/videoDetail/4b1141305d9cd231015d9d0992ef0030
[19]:https://zhuanlan.zhihu.com/p/39191116
[20]:http://zake7749.github.io/2017/09/28/Sequence-to-Sequence-tutorial/
[21]:https://docs.google.com/presentation/d/e/2PACX-1vSVi_bYmbEMRDYON1sJJZfcG2KidmPjh-X7zeoNn7s6eidzgdJgOJevW3xxE2dhO1i6qN-OBM8tnHpq/pub?start=false&loop=false&delayms=3000
[22]:https://www.youtube.com/watch?v=hAA_AfVB89M&app=desktop
[23]:https://docs.google.com/presentation/d/e/2PACX-1vRjpbhBYS6YsE_d03gDK65cWTKFsyko_2XaOCKMYWbPnjZo8eieCExmwycm7sNCuNNb0aiofW96819z/pub?start=false&loop=false&delayms=3000
[24]:https://github.com/chenyuntc/pytorch-book/tree/master/chapter9-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%86%99%E8%AF%97(CharRNN)
